{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlmuriwXt0+TfrEEwF16C4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjadon/llm/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under the hood, pipeline does the following\n",
        "\n",
        "- Load model + tokenizer from the hub\n",
        "- Convert input into tokens\n",
        "- Run through the model\n",
        "- Convert output to human-readable text\n",
        "- Return structured output (JSON-like)"
      ],
      "metadata": {
        "id": "EautBIMq04jL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline is like a fully automatic washing machine.\n",
        "You don't need to know drum rotation speed (model internals).\n",
        "You choose a program (task), give input (clothes), and get the result (clean output)."
      ],
      "metadata": {
        "id": "7N6Mf9pJ6PJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "RdaNOhG26WC3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "9xxTU-DJL30d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1vpBohg4LF2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6052d8-2bc3-425a-fbf7-c282ed068bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff3829d0",
        "outputId": "2c1069a6-bfdb-45d3-cd66-a40e1c97bb14"
      },
      "source": [
        "print(classifier.model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier.tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vEcbYIE6dSq",
        "outputId": "df2ba449-345c-4015-a010-35493ac1b11c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = pipeline(\"sentiment-analysis\")\n",
        "sentiment([\n",
        "    \"This class is fantastic!\",\n",
        "    \"I hate slow WiFi.\"\n",
        "])"
      ],
      "metadata": {
        "id": "Ul7RSys0zdEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment.model)"
      ],
      "metadata": {
        "id": "YPa_PFM50jSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pipeline(\n",
        "    task,\n",
        "    model=None,\n",
        "    tokenizer=None,\n",
        "    framework=None,\n",
        "    device=-1,\n",
        "    **kwargs\n",
        ")`"
      ],
      "metadata": {
        "id": "08elEJYO1QK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"gpt2\")"
      ],
      "metadata": {
        "id": "rZGbiaW71bFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"AI in education will\", max_new_tokens=20)"
      ],
      "metadata": {
        "id": "A1AcyoEY1na8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "\n",
        "translator(\"Students love learning new technologies.\")\n"
      ],
      "metadata": {
        "id": "gPt_bdy-15s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "text = \"\"\"LLMs are transforming education by enabling intelligent tutoring,\n",
        "automated grading, content creation, and personalized learning.\"\"\"\n",
        "summarizer(text)\n"
      ],
      "metadata": {
        "id": "hG07DFHY2HH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer(text, max_length=10)"
      ],
      "metadata": {
        "id": "TWBEhKu92day"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "image = Image.open(requests.get(\"https://huggingface.co/datasets/mishig/sample_images/resolve/main/panda.jpg\", stream=True).raw)\n",
        "classifier = pipeline(\"image-classification\")\n",
        "\n",
        "classifier(image)\n"
      ],
      "metadata": {
        "id": "aG1RtRgo5BK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = pipeline(\n",
        "    task=\"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "sentiment(\"Great teaching!\")\n"
      ],
      "metadata": {
        "id": "QIKrM-He5aMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "nlp(\"Transformers are amazing.\")\n"
      ],
      "metadata": {
        "id": "16gBn2Y55lTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment.save_pretrained(\"my_pipeline\")\n",
        "\n",
        "from transformers import pipeline\n",
        "loaded = pipeline(\"sentiment-analysis\", model=\"my_pipeline\")\n"
      ],
      "metadata": {
        "id": "Q-Jf_Brm5waS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step of our pipeline is to convert the text inputs into numbers (tokenization) that the model can make sense of. So we first download that information from the Model using AutoTokenizer class and its from_pretrained() method."
      ],
      "metadata": {
        "id": "F5KedC-QNvrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "lWbo7hNdOd2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "AbdNS1BoNyJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can directly pass our sentences to it to get a dictionary that is ready to feed to our model."
      ],
      "metadata": {
        "id": "F4iQpyzEOxtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "NBUZxL77OmCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "BhGlDESGPFHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a dictionary containing two keys, input_ids and attention_mask. input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence."
      ],
      "metadata": {
        "id": "7YrzF2F_PYjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "P3xeh1gGoFhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "NekEM01kPdhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "dToaBgl8oKL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We are getting torch.Size tensor because the Hugging Face Transformers model internally uses PyTorch, even if you didn't import it.**\n",
        "\n",
        "The returned object of (outputs.last_hidden_state) is a PyTorch tensor."
      ],
      "metadata": {
        "id": "7WwvJxDXqDzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]"
      ],
      "metadata": {
        "id": "lvOH5VxXq1Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[\"last_hidden_state\"].shape[0]"
      ],
      "metadata": {
        "id": "inGlzYQvqOAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModel = Base Transformer (No Task Head)\n",
        "This gives you only the core Transformer ‚Äî the encoder/decoder blocks\n",
        "\n",
        "Only hidden states Useful for embeddings, representation learning, similarity, clustering, etc.\n",
        "\n",
        "Task-Specific Models = AutoModel + Task Head\n",
        "\n",
        "Each class like AutoModelForCausalLM, AutoModelForSequenceClassification, etc. adds a head on top of the base model.\n",
        "\n",
        "That means:\n",
        "\n",
        "AutoModel = backbone\n",
        "AutoModelForXxx = backbone + task layer\n",
        "\n"
      ],
      "metadata": {
        "id": "jsa7TEoaGn_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModel is Bare Transformer (BERT encoder or GPT decoder)\n",
        "\n",
        "Use when You want token embeddings, You want sentence embeddings, You want to do cosine similarity or clustering, You are building your own model head manually"
      ],
      "metadata": {
        "id": "Rzn2eIER0Yrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForCausalLM For text generation tasks (GPT-style models)\n",
        "\n",
        "What it adds: A causal language modeling head that predicts the next token using left-to-right attention.\n",
        "\n",
        "Use when: Chatbots, Story generation, Code generation, Autocomplete"
      ],
      "metadata": {
        "id": "O7GzjH0I0vWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForMaskedLM\n",
        "\n",
        "‚û°Ô∏è For fill-in-the-blanks tasks (BERT-style models)\n",
        "\n",
        "What it adds:\n",
        "\n",
        "A masked language modeling head that predicts masked tokens ([MASK]).\n",
        "\n",
        "Use when:\n",
        "\n",
        "Fill in missing words\n",
        "\n",
        "Pretraining-like tasks\n",
        "\n",
        "Masked token prediction"
      ],
      "metadata": {
        "id": "PKLD6_hR04XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForSequenceClassification\n",
        "\n",
        "‚û°Ô∏è For sentence-level classification\n",
        "\n",
        "What it adds:\n",
        "\n",
        "A classification head on top of the [CLS] token.\n",
        "\n",
        "Use when:\n",
        "\n",
        "Sentiment analysis\n",
        "\n",
        "Spam detection\n",
        "\n",
        "Intent classification\n",
        "\n",
        "Fake news detection"
      ],
      "metadata": {
        "id": "OojUX_il07Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "XcZFKjV51xOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits.shape)\n"
      ],
      "metadata": {
        "id": "UV7j0jo92KcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):\n",
        "\n",
        "The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look:\n",
        "\n"
      ],
      "metadata": {
        "id": "Wcz66uZV2T4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "ZrnfnskI2c3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
      ],
      "metadata": {
        "id": "SP6EjHbr2mkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "vJNqA6fx2ofW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that the model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one. These are recognizable probability scores.\n",
        "\n",
        "To get the labels corresponding to each position, we can inspect the id2label attribute of the model config (more on this in the next section):\n",
        "\n",
        "Copied\n"
      ],
      "metadata": {
        "id": "cPKazCCv2xZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "id": "trQsjaWB2ywb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing! Now let‚Äôs take some time to dive deeper into each of those steps."
      ],
      "metadata": {
        "id": "GzC_eUBp2-O2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForTokenClassification\n",
        "\n",
        "‚û°Ô∏è For tagging each token\n",
        "\n",
        "What it adds:\n",
        "\n",
        "A head that produces a label per token.\n",
        "\n",
        "Use when:\n",
        "\n",
        "Named Entity Recognition (NER)\n",
        "\n",
        "Parts-of-speech tagging (POS)\n",
        "\n",
        "Chunking"
      ],
      "metadata": {
        "id": "Kk4gM_T41KHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForQuestionAnswering\n",
        "\n",
        "‚û°Ô∏è For extractive QA (SQuAD-style)\n",
        "\n",
        "What it adds:\n",
        "\n",
        "Two heads:\n",
        "\n",
        "Start position of answer\n",
        "\n",
        "End position of answer\n",
        "\n",
        "Use when:\n",
        "\n",
        "Reading comprehension\n",
        "\n",
        "Extracting answers from passages"
      ],
      "metadata": {
        "id": "GuGywVAb1GAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "utoModelForMultipleChoice\n",
        "\n",
        "‚û°Ô∏è For MCQs (multiple-choice questions)\n",
        "\n",
        "What it adds:\n",
        "\n",
        "A head that compares several options.\n",
        "\n",
        "Example:\n",
        "\n",
        "For 4 options:\n",
        "se when:\n",
        "\n",
        "AI answering exam-type MCQs\n",
        "\n",
        "Sentence reasoning choices"
      ],
      "metadata": {
        "id": "vb14ZrqM1cmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForSeq2SeqLM\n",
        "\n",
        "‚û°Ô∏è For encoder‚Äìdecoder models (translation, summarization)\n",
        "\n",
        "Use when:\n",
        "\n",
        "Translation\n",
        "\n",
        "Summarization\n",
        "\n",
        "Paraphrasing"
      ],
      "metadata": {
        "id": "keMHiR6W1ayI"
      }
    }
  ]
}
