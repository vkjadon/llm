{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM941XrF7at1LOTL6SLEyCV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjadon/llm/blob/main/02Token.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from transformers import BertTokenizer\n",
        "from transformers import XLNetTokenizer\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YadiG0wUXIC",
        "outputId": "ee8e782d-33e3-4ae2-d217-0df85c50c55a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample sentence for word tokenization.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT4uH2B-VlDN",
        "outputId": "160ab4bc-8e2e-4eef-cc66-308daca2bd59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This showcases word_tokenize from nltk library\n",
        "\n",
        "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQEQCvS0Vmlo",
        "outputId": "615593d5-9df5-4a60-d3db-fe853643b63e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
        "\n",
        "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Making a list of the tokens and priting the list\n",
        "token_list = [token.text for token in doc]\n",
        "print(\"Tokens:\", token_list)\n",
        "\n",
        "# Showing token details\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kT8HLmXVtt5",
        "outputId": "023e22c6-ad09-4d4e-f762-dcc3a2d0cee2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n",
            "I PRON nsubj\n",
            "could AUX aux\n",
            "n't PART neg\n",
            "help VERB ROOT\n",
            "the DET det\n",
            "dog NOUN dobj\n",
            ". PUNCT punct\n",
            "Ca AUX aux\n",
            "n't PART neg\n",
            "you PRON nsubj\n",
            "do VERB ROOT\n",
            "it PRON dobj\n",
            "? PUNCT punct\n",
            "Do AUX aux\n",
            "n't PART neg\n",
            "be AUX ROOT\n",
            "afraid ADJ acomp\n",
            "if SCONJ mark\n",
            "you PRON nsubj\n",
            "are AUX advcl\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl4nZ0GpcKfi",
        "outputId": "d11484c1-fbef-45df-a468-393cb49c78c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('This', 'I'), ('is', 'love'), ('bad', 'PyTorch')], tensor([0, 1])]\n"
          ]
        }
      ]
    }
  ]
}