{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfTCEAf/7vZ/Kyt28gL5I5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjadon/llm/blob/main/03hf_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiJQ1lt8suJO"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(transformers)"
      ],
      "metadata": {
        "id": "nP9aKr8gtBmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first type of tokenizer that comes to mind is word-based. It's generally very easy to set up and use with only a few rules, and it often yields decent results. There are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python's split() function\n"
      ],
      "metadata": {
        "id": "lglUr_oLKhrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "lDHAQpSmKtXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word-level tokenizers split text into whole words, sometimes using extra rules for punctuation. This creates very large vocabularies, because each unique word in the language must have its own ID. English alone has over 500,000 words, and forms like dog/dogs or run/running are treated as completely different tokens, so the model initially doesn't know they are related.\n",
        "\n",
        "Because no vocabulary can include every possible word, tokenizers need an unknown token (often [UNK] or <unk>). When a tokenizer produces many unknown tokens, it means it is failing to represent words properly and losing information. Therefore, vocabularies should be designed to minimize unknown tokens while keeping the vocabulary size manageable."
      ],
      "metadata": {
        "id": "g_FL4JZmKssT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to reduce the amount of unknown tokens is to go one level deeper, using a character-based tokenizer."
      ],
      "metadata": {
        "id": "z704C-KwLWBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character-based tokenizers split text into individual characters instead of words. This greatly reduces vocabulary size and almost eliminates unknown tokens, since all words can be formed from characters. However, this approach introduces challenges with handling spaces and punctuation, and the tokens carry less meaning, especially in languages like English (though not in languages like Chinese, where characters are meaningful). It also increases sequence length because one word becomes many character tokens.\n",
        "\n",
        "To balance the advantages of word- and character-level methods, subword tokenization is used.\n",
        "\n",
        "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
        "\n",
        "For instance, “annoyingly” might be considered a rare word and could be decomposed into “annoying” and “ly”. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of “annoyingly” is kept by the composite meaning of “annoying” and “ly”.\n",
        "\n",
        "Here is an example showing how a subword tokenization algorithm would tokenize the sequence “Let's do tokenization!“"
      ],
      "metadata": {
        "id": "EWdB-P3KLYD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:"
      ],
      "metadata": {
        "id": "Ny8WiO3xPTak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer(\"Using a Transformer network is simple\")"
      ],
      "metadata": {
        "id": "MzIqEeg9PWbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenizer))"
      ],
      "metadata": {
        "id": "eerWelGpP0R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n",
        "\n",
        "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:"
      ],
      "metadata": {
        "id": "EeDYDqGLPjI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "tokenizer(\"Using a Transformer network is simple\")"
      ],
      "metadata": {
        "id": "ocRzbLmqPZEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenizer))"
      ],
      "metadata": {
        "id": "so1F_07DQY7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "text = \"Transformers are powerful!\"\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "ids = tokenizer.encode(text)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"IDs:\", ids)"
      ],
      "metadata": {
        "id": "gtsoDkR72xRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "F6dt3kzxtMG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(AutoTokenizer.from_pretrained)"
      ],
      "metadata": {
        "id": "aTO14cYQtR0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TOKENIZER_MAPPING\n",
        "\n",
        "list(TOKENIZER_MAPPING.keys())"
      ],
      "metadata": {
        "id": "MRopn5pXx8qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_classes = [\n",
        "    cls.__name__\n",
        "    for cls in transformers.__dict__.values()\n",
        "    if isinstance(cls, type) and \"Tokenizer\" in cls.__name__\n",
        "]\n",
        "\n",
        "tokenizer_classes"
      ],
      "metadata": {
        "id": "ZrioL6zqyT8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc2832ab"
      },
      "source": [
        "First, let's load a pre-trained tokenizer, for example, from the 'bert-base-uncased' model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34a9f4d8"
      },
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# print(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79ca111"
      },
      "source": [
        "Now, let's use this tokenizer to encode a sample sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f994950b"
      },
      "source": [
        "sentence = \"Hello, how are you today?\"\n",
        "encoded_input = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "print(\"Encoded input:\", encoded_input)\n",
        "print(\"Input IDs:\", encoded_input['input_ids'])\n",
        "print(\"Attention Mask:\", encoded_input['attention_mask'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello\"\n",
        "encoded_input = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "print(\"Encoded input:\", encoded_input)\n",
        "print(\"Input IDs:\", encoded_input['input_ids'])\n",
        "print(\"Attention Mask:\", encoded_input['attention_mask'])"
      ],
      "metadata": {
        "id": "ZNOXKHQfllFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f46cee5"
      },
      "source": [
        "We can also decode the input IDs back to text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c47856a"
      },
      "source": [
        "decoded_output = tokenizer.decode(encoded_input['input_ids'][0])\n",
        "print(\"Decoded output:\", decoded_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"Unbelievable performance at JadonTechLabs!\"\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "ids = tokenizer.encode(text)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", ids)"
      ],
      "metadata": {
        "id": "jnxb3Uja2oym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}