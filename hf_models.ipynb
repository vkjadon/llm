{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDeM9hxfxVpsgwEDqw1q/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjadon/llm/blob/main/hf_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `AutoModel` class from the `transformers` library to download and cache the specific model architecture and weights using `from_pretrained()` method. The AutoModel class is a wrappers designed to fetch the appropriate model architecture for a given checkpoint."
      ],
      "metadata": {
        "id": "hyVx3t6KKeaC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_p5zEIH3WSZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case AutoModel will fetch a BERT model on the basis of the checkpoint provided in the `from_pretrained()` method. It downloads and chches the model architechure (12 layers, 768 hidden size, 12 attention heads) and the weights from HuggingFace Hub.\n",
        "\n",
        "However, we can use a specific model class directly in case we know the type of model we want to use for the checkpoint."
      ],
      "metadata": {
        "id": "fWv94u1HLMrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "4ba5MPxS3gwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and saving\n",
        "\n",
        "We can use `save_pretrained()` method, to save the model's weights and architecture configuration."
      ],
      "metadata": {
        "id": "RgJcGsHuNj1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"bert\")"
      ],
      "metadata": {
        "id": "eFDR2yUP6hDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will save two file in the path provided in the `save_pretrained()`. The path will have two files `config.json` and `model.safetensors`."
      ],
      "metadata": {
        "id": "JZ3EPrP8OG3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The config.json file have necessary attributes needed to build the model architecture and some metadata. The `model.safetensors` is the state dictionary containing weights.\n",
        "\n",
        "To reuse a saved model, use the from_pretrained() method again."
      ],
      "metadata": {
        "id": "lNkJWnrFO2WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"bert\")"
      ],
      "metadata": {
        "id": "G9KatFwq66jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Flow\n",
        "\n",
        "Transformer models handle text by turning the inputs into numbers. Here we will look at exactly what happens when your text is processed by the tokenizer."
      ],
      "metadata": {
        "id": "B6FsDSFJfwkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "XVpEvAGt7R8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can decode the input IDs to get back the original text"
      ],
      "metadata": {
        "id": "cwc0FZudgIzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "hEOmtv4H7T1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll notice that the tokenizer has added special tokens — [CLS] and [SEP] — required by the model. Not all models need special tokens; they’re utilized when a model was pretrained with them"
      ],
      "metadata": {
        "id": "mjPXxeOcgVcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer([\"How are you?\", \"I'm fine, thank you!\"])\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "bYfdrsUV7aud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us ask the tokenizer to return tensors directly from PyTorch"
      ],
      "metadata": {
        "id": "LHljWqpslb1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    [\"How are you?\", \"I'm fine, thank you!\"],\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "Jyx3lDJf7exV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you ask the tokenizer to return PyTorch tensors (return_tensors=\"pt\"), it needs all sequences to be of the same length to form a batch. To fix this, you need to explicitly tell the tokenizer to pad the shorter sequences so they all match the length of the longest one. You can do this by adding padding=True to your tokenizer call."
      ],
      "metadata": {
        "id": "tUlL9IjZkkPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    [\"How are you?\", \"I'm fine, thank you!\"], padding=True, return_tensors=\"pt\"\n",
        ")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "obfHmlxn7nui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the padding tokens have been encoded into input IDs with ID 0, and they have an attention mask value of 0 as well. This is because those padding tokens shouldn't be analyzed by the model: they're not part of the actual sentence."
      ],
      "metadata": {
        "id": "M7BojSfwlsQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tensors might get too big to be processed by the model. For instance, BERT was only pretrained with sequences up to 512 tokens, so it cannot process longer sequences. If you have sequences longer than the model can handle, you’ll need to truncate them with the truncation parameter:"
      ],
      "metadata": {
        "id": "ad8hvBuBl-gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "XZnooR-u7rnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c4dd37-d7fb-4328-fd31-678021941417"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1188, 1110,  170, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304,\n",
            "         1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304,\n",
            "         1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304,\n",
            "         1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304,\n",
            "         1304, 1304, 1304, 1304, 1304, 1263, 5650,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the tokenizer truncates only if your input exceeds the model's default maximum length. For model_max_length for BERT is 512 tokens, for GPT-2 is 1024 for LLaMA it is 2048, 4096, etc.\n",
        "We can explicitly set maximum length by using `max_length` parameter."
      ],
      "metadata": {
        "id": "s2uU1NYYnMWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    [\"How are you?\", \"I'm fine, thank you!\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=5,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "WXssXZ7B7s_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cc5c0b-047c-49df-e721-9c7e21139e2f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1731, 1132, 1128,  102],\n",
            "        [ 101,  146,  112,  182,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uSLM9Id9nx8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"How are you?\")\n",
        "print(encoded_input[\"input_ids\"])\n",
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "bD3-e83D7xQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]"
      ],
      "metadata": {
        "id": "1lJX8q9f7ybf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}