{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPorAF8oqMc3HHLBObgAPfc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjadon/llm/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "9xxTU-DJL30d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jeI20kByNuIt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vpBohg4LF2I"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step of our pipeline is to convert the text inputs into numbers (tokenization) that the model can make sense of. So we first download that information from the Model using AutoTokenizer class and its from_pretrained() method."
      ],
      "metadata": {
        "id": "F5KedC-QNvrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "lWbo7hNdOd2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "AbdNS1BoNyJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can directly pass our sentences to it to get a dictionary that is ready to feed to our model."
      ],
      "metadata": {
        "id": "F4iQpyzEOxtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "NBUZxL77OmCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "BhGlDESGPFHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a dictionary containing two keys, input_ids and attention_mask. input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence."
      ],
      "metadata": {
        "id": "7YrzF2F_PYjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "P3xeh1gGoFhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "NekEM01kPdhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "dToaBgl8oKL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We are getting torch.Size tensor because the Hugging Face Transformers model internally uses PyTorch, even if you didn't import it.**\n",
        "\n",
        "The returned object of (outputs.last_hidden_state) is a PyTorch tensor."
      ],
      "metadata": {
        "id": "7WwvJxDXqDzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]"
      ],
      "metadata": {
        "id": "lvOH5VxXq1Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[\"last_hidden_state\"].shape[0]"
      ],
      "metadata": {
        "id": "inGlzYQvqOAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModel = Base Transformer (No Task Head)\n",
        "This gives you only the core Transformer ‚Äî the encoder/decoder blocks\n",
        "\n",
        "Only hidden states Useful for embeddings, representation learning, similarity, clustering, etc.\n",
        "\n",
        "Task-Specific Models = AutoModel + Task Head\n",
        "\n",
        "Each class like AutoModelForCausalLM, AutoModelForSequenceClassification, etc. adds a head on top of the base model.\n",
        "\n",
        "That means:\n",
        "\n",
        "AutoModel = backbone\n",
        "AutoModelForXxx = backbone + task layer\n",
        "\n"
      ],
      "metadata": {
        "id": "jsa7TEoaGn_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModel is Bare Transformer (BERT encoder or GPT decoder)\n",
        "\n",
        "Use when You want token embeddings, You want sentence embeddings, You want to do cosine similarity or clustering, You are building your own model head manually"
      ],
      "metadata": {
        "id": "Rzn2eIER0Yrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForCausalLM For text generation tasks (GPT-style models)\n",
        "\n",
        "What it adds: A causal language modeling head that predicts the next token using left-to-right attention.\n",
        "\n",
        "Use when: Chatbots, Story generation, Code generation, Autocomplete"
      ],
      "metadata": {
        "id": "O7GzjH0I0vWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForMaskedLM\n",
        "\n",
        "‚û°Ô∏è For fill-in-the-blanks tasks (BERT-style models)\n",
        "\n",
        "What it adds:\n",
        "\n",
        "A masked language modeling head that predicts masked tokens ([MASK]).\n",
        "\n",
        "Use when:\n",
        "\n",
        "Fill in missing words\n",
        "\n",
        "Pretraining-like tasks\n",
        "\n",
        "Masked token prediction"
      ],
      "metadata": {
        "id": "PKLD6_hR04XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForSequenceClassification\n",
        "\n",
        "‚û°Ô∏è For sentence-level classification\n",
        "\n",
        "What it adds:\n",
        "\n",
        "A classification head on top of the [CLS] token.\n",
        "\n",
        "Use when:\n",
        "\n",
        "Sentiment analysis\n",
        "\n",
        "Spam detection\n",
        "\n",
        "Intent classification\n",
        "\n",
        "Fake news detection"
      ],
      "metadata": {
        "id": "OojUX_il07Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "XcZFKjV51xOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits.shape)\n"
      ],
      "metadata": {
        "id": "UV7j0jo92KcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):\n",
        "\n",
        "The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look:\n",
        "\n"
      ],
      "metadata": {
        "id": "Wcz66uZV2T4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "ZrnfnskI2c3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
      ],
      "metadata": {
        "id": "SP6EjHbr2mkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "vJNqA6fx2ofW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that the model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one. These are recognizable probability scores.\n",
        "\n",
        "To get the labels corresponding to each position, we can inspect the id2label attribute of the model config (more on this in the next section):\n",
        "\n",
        "Copied\n"
      ],
      "metadata": {
        "id": "cPKazCCv2xZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "id": "trQsjaWB2ywb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing! Now let‚Äôs take some time to dive deeper into each of those steps."
      ],
      "metadata": {
        "id": "GzC_eUBp2-O2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForTokenClassification\n",
        "\n",
        "‚û°Ô∏è For tagging each token\n",
        "\n",
        "What it adds:\n",
        "\n",
        "A head that produces a label per token.\n",
        "\n",
        "Use when:\n",
        "\n",
        "Named Entity Recognition (NER)\n",
        "\n",
        "Parts-of-speech tagging (POS)\n",
        "\n",
        "Chunking"
      ],
      "metadata": {
        "id": "Kk4gM_T41KHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForQuestionAnswering\n",
        "\n",
        "‚û°Ô∏è For extractive QA (SQuAD-style)\n",
        "\n",
        "What it adds:\n",
        "\n",
        "Two heads:\n",
        "\n",
        "Start position of answer\n",
        "\n",
        "End position of answer\n",
        "\n",
        "Use when:\n",
        "\n",
        "Reading comprehension\n",
        "\n",
        "Extracting answers from passages"
      ],
      "metadata": {
        "id": "GuGywVAb1GAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "utoModelForMultipleChoice\n",
        "\n",
        "‚û°Ô∏è For MCQs (multiple-choice questions)\n",
        "\n",
        "What it adds:\n",
        "\n",
        "A head that compares several options.\n",
        "\n",
        "Example:\n",
        "\n",
        "For 4 options:\n",
        "se when:\n",
        "\n",
        "AI answering exam-type MCQs\n",
        "\n",
        "Sentence reasoning choices"
      ],
      "metadata": {
        "id": "vb14ZrqM1cmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoModelForSeq2SeqLM\n",
        "\n",
        "‚û°Ô∏è For encoder‚Äìdecoder models (translation, summarization)\n",
        "\n",
        "Use when:\n",
        "\n",
        "Translation\n",
        "\n",
        "Summarization\n",
        "\n",
        "Paraphrasing"
      ],
      "metadata": {
        "id": "keMHiR6W1ayI"
      }
    }
  ]
}